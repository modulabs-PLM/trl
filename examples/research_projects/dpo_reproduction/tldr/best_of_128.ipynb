{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ch_conda\\envs\\choi_llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\ch_conda\\envs\\choi_llm\\lib\\site-packages\\trl\\trainer\\ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.36s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): GPTJForCausalLM(\n",
       "    (transformer): GPTJModel(\n",
       "      (wte): Embedding(50400, 4096)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0-27): 28 x GPTJBlock(\n",
       "          (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): GPTJAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "          (mlp): GPTJMLP(\n",
       "            (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "            (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "            (act): NewGELUActivation()\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=4096, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = 'EleutherAI/gpt-j-6b'\n",
    "ppo_model_dir = 'CarperAI/openai_summarize_tldr_ppo'\n",
    "sft_model_dir = 'CarperAI/openai_summarize_tldr_sft'\n",
    "reward_model_dir = 'cjhyeok/tldr-reward_model'\n",
    "\n",
    "#모델 저장하고 쓴것 위  경로가 맞아요\n",
    "# ppo_model_dir = './tldr_ppo'\n",
    "# sft_model_dir = './tldr_sft'\n",
    "# reward_model_dir = './tldr_reward' \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(ppo_model_dir)\n",
    "sft_model = AutoModelForCausalLMWithValueHead.from_pretrained(sft_model_dir)\n",
    "reward_model = pipeline(\"text-classification\", model=reward_model_dir, tokenizer=tokenizer)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# cuda-ize models\n",
    "ppo_model.cuda()\n",
    "sft_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 532/532 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 111M/111M [00:25<00:00, 4.26MB/s]\n",
      "Downloading data: 100%|██████████| 6.23M/6.23M [00:01<00:00, 3.93MB/s]\n",
      "Downloading data: 100%|██████████| 6.12M/6.12M [00:01<00:00, 4.04MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:29<00:00,  9.70s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 159.35it/s]\n",
      "Generating train split: 100%|██████████| 116722/116722 [00:00<00:00, 271610.92 examples/s]\n",
      "Generating test split: 100%|██████████| 6553/6553 [00:00<00:00, 361258.56 examples/s]\n",
      "Generating valid split: 100%|██████████| 6447/6447 [00:00<00:00, 393078.82 examples/s]\n",
      "Filter: 100%|██████████| 116722/116722 [00:00<00:00, 241258.51 examples/s]\n",
      "Map: 100%|██████████| 116528/116528 [01:40<00:00, 1159.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(tokenizer, dataset_name=\"CarperAI/openai_summarize_tldr\", input_min_text_length=2, input_max_text_length=8):\n",
    "    # load imdb with datasets\n",
    "    ds = load_dataset(dataset_name, split=\"train\")\n",
    "    ds = ds.rename_columns({\"prompt\": \"review\"})\n",
    "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
    "\n",
    "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "dataset = build_dataset(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}\n",
    "sent_kwargs = {\"top_k\": None, \"function_to_apply\": \"none\", \"batch_size\": 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_min_length = 4\n",
    "output_max_length = 16\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "#### get a batch from the dataset\n",
    "bs = 16\n",
    "output_data = dict()\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:].sample(bs)\n",
    "output_data[\"query\"] = df_batch[\"query\"].tolist()\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "# :: [Resp]\n",
    "response_tensors_ref, response_tensors = [], []\n",
    "# :: [[Resp]]\n",
    "response_tensors_best_of = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:29<00:00,  9.35s/it]\n"
     ]
    }
   ],
   "source": [
    "N_BEST_OF =128\n",
    "for i in tqdm(range(bs)):\n",
    "    gen_len = output_length_sampler()\n",
    "\n",
    "    query = torch.tensor(query_tensors[i])\n",
    "\n",
    "    output = ppo_model.generate(query.unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs).squeeze()\n",
    "    response_tensors_ref.append(tokenizer.decode(output))\n",
    "\n",
    "    output = sft_model.generate(query.unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs).squeeze()\n",
    "    response_tensors.append(tokenizer.decode(output))\n",
    "\n",
    "    # generating copies of the same query for the Best-of-n sampling\n",
    "    queries = query.repeat((N_BEST_OF, 1))\n",
    "    output = ppo_model.generate(queries.to(device), max_new_tokens=gen_len, **gen_kwargs).squeeze()\n",
    "    response_tensors_best_of.append(tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [14:12, 53.27s/it]\n"
     ]
    }
   ],
   "source": [
    "scores_ref = [output[0][\"score\"] for output in reward_model(response_tensors_ref, **sent_kwargs)]\n",
    "scores = [output[0][\"score\"] for output in reward_model(response_tensors, **sent_kwargs)]\n",
    "scores_best_of = []\n",
    "for i, response in tqdm(enumerate(response_tensors_best_of)):\n",
    "    # base_score = scores_ref[i]\n",
    "    scores_best_of.append(torch.tensor([output[0][\"score\"] for output in reward_model(response, **sent_kwargs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>response (ref)</th>\n",
       "      <th>scores (ref)</th>\n",
       "      <th>response (RLHF)</th>\n",
       "      <th>scores (RLHF)</th>\n",
       "      <th>response (best_of)</th>\n",
       "      <th>scores (best_of)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>SUBREDDIT: r/dating_advice\\nelsenworth</td>\n",
       "      <td>-2.726518</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nTITLE: Went</td>\n",
       "      <td>-0.719674</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n classrooms is this no...</td>\n",
       "      <td>2.801826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>SUBREDDIT: r/relationships\\n income</td>\n",
       "      <td>1.748880</td>\n",
       "      <td>SUBREDDIT: r/dogs\\nTITLE</td>\n",
       "      <td>0.305112</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n explains</td>\n",
       "      <td>2.208817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUBRED</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nяп</td>\n",
       "      <td>-2.728848</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nTITLE:</td>\n",
       "      <td>-1.110452</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n checking app capabili...</td>\n",
       "      <td>3.762428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUBREDDIT:</td>\n",
       "      <td>SUBREDDIT: r/relationships\\n royal blood, marr...</td>\n",
       "      <td>1.197688</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nTITLE: My [20 M] F...</td>\n",
       "      <td>0.465835</td>\n",
       "      <td>SUBREDDIT: r/Dogtraining\\n Various aspects of ...</td>\n",
       "      <td>4.343467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SUBRED</td>\n",
       "      <td>SUBREDDIT: r/relationship_advice\\nMake a frien...</td>\n",
       "      <td>-2.282068</td>\n",
       "      <td>SUBREDDIT: r/tifu\\nTITLE: TIFU by accident</td>\n",
       "      <td>-0.099994</td>\n",
       "      <td>SUBREDDIT: r/relationships\\n\"...If your hunger...</td>\n",
       "      <td>2.549654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SUBREDDIT: r</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n followsreddiquette=!!...</td>\n",
       "      <td>-0.283909</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nTITLE: I [28 M</td>\n",
       "      <td>0.815666</td>\n",
       "      <td>SUBREDDIT: r/relationship_advice\\n underwent a...</td>\n",
       "      <td>2.639867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SUBRED</td>\n",
       "      <td>SUBREDDIT: r/jobs\\naww</td>\n",
       "      <td>-2.568755</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTIT</td>\n",
       "      <td>1.383220</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n configured</td>\n",
       "      <td>3.596242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SUBREDDIT:</td>\n",
       "      <td>SUBREDDIT: r/relationships\\n JOHN</td>\n",
       "      <td>-1.511305</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nTIT</td>\n",
       "      <td>0.939206</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n 389</td>\n",
       "      <td>2.758214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SUBREDDIT: r</td>\n",
       "      <td>SUBREDDIT: r/tifu\\n\\nTIFU by</td>\n",
       "      <td>-1.859813</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTITLE: My girlfriend</td>\n",
       "      <td>0.701582</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n by venturebubb\\n</td>\n",
       "      <td>2.781665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>SUBREDDIT: r/cats\\n poses A [f] physical confr...</td>\n",
       "      <td>-1.856711</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTITLE: How do you expl...</td>\n",
       "      <td>-0.890963</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n (\"Random curiosities ...</td>\n",
       "      <td>2.662919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SUB</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\n indicators KushEx</td>\n",
       "      <td>-1.991775</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTITLE:</td>\n",
       "      <td>1.354235</td>\n",
       "      <td>SUBREDDIT: r/personalfinance\\nTitles</td>\n",
       "      <td>4.292090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>SUBREDDIT: r/tifu\\n twice\\n</td>\n",
       "      <td>-0.259468</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTITLE</td>\n",
       "      <td>2.002223</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nPartner</td>\n",
       "      <td>4.770880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SUBREDDIT: r</td>\n",
       "      <td>SUBREDDIT: r/relationships\\n magical Urban Qui...</td>\n",
       "      <td>-0.089236</td>\n",
       "      <td>SUBREDDIT: r/relationships\\nTITLE: My family w...</td>\n",
       "      <td>0.227591</td>\n",
       "      <td>SUBREDDIT: r/running\\ndiet\\nTITLE: Overweight</td>\n",
       "      <td>2.343759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SUBRED</td>\n",
       "      <td>SUBREDDIT: r/travel\\n\\nMy sister and I</td>\n",
       "      <td>-0.571043</td>\n",
       "      <td>SUBREDDIT: r/legaladvice\\nTITLE:</td>\n",
       "      <td>-0.949782</td>\n",
       "      <td>SUBREDDIT: r/relationship_advice\\n except</td>\n",
       "      <td>3.176759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SUBREDDIT:</td>\n",
       "      <td>SUBREDDIT: r/selfdestroyed\\n Wizard:</td>\n",
       "      <td>-0.235847</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTITLE:</td>\n",
       "      <td>1.354235</td>\n",
       "      <td>SUBREDDIT: r/AskReddit\\nTitle: As</td>\n",
       "      <td>4.721159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SUBRED</td>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>-0.965613</td>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>-0.965613</td>\n",
       "      <td>SUBREDDIT: r/</td>\n",
       "      <td>-0.965613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            query                                     response (ref)  \\\n",
       "0   SUBREDDIT: r/             SUBREDDIT: r/dating_advice\\nelsenworth   \n",
       "1   SUBREDDIT: r/                SUBREDDIT: r/relationships\\n income   \n",
       "2          SUBRED                     SUBREDDIT: r/relationships\\nяп   \n",
       "3      SUBREDDIT:  SUBREDDIT: r/relationships\\n royal blood, marr...   \n",
       "4          SUBRED  SUBREDDIT: r/relationship_advice\\nMake a frien...   \n",
       "5    SUBREDDIT: r  SUBREDDIT: r/AskReddit\\n followsreddiquette=!!...   \n",
       "6          SUBRED                             SUBREDDIT: r/jobs\\naww   \n",
       "7      SUBREDDIT:                  SUBREDDIT: r/relationships\\n JOHN   \n",
       "8    SUBREDDIT: r                       SUBREDDIT: r/tifu\\n\\nTIFU by   \n",
       "9   SUBREDDIT: r/  SUBREDDIT: r/cats\\n poses A [f] physical confr...   \n",
       "10            SUB         SUBREDDIT: r/AskReddit\\n indicators KushEx   \n",
       "11  SUBREDDIT: r/                        SUBREDDIT: r/tifu\\n twice\\n   \n",
       "12   SUBREDDIT: r  SUBREDDIT: r/relationships\\n magical Urban Qui...   \n",
       "13         SUBRED             SUBREDDIT: r/travel\\n\\nMy sister and I   \n",
       "14     SUBREDDIT:               SUBREDDIT: r/selfdestroyed\\n Wizard:   \n",
       "15         SUBRED                                      SUBREDDIT: r/   \n",
       "\n",
       "    scores (ref)                                    response (RLHF)  \\\n",
       "0      -2.726518            SUBREDDIT: r/relationships\\nTITLE: Went   \n",
       "1       1.748880                           SUBREDDIT: r/dogs\\nTITLE   \n",
       "2      -2.728848                 SUBREDDIT: r/relationships\\nTITLE:   \n",
       "3       1.197688  SUBREDDIT: r/relationships\\nTITLE: My [20 M] F...   \n",
       "4      -2.282068         SUBREDDIT: r/tifu\\nTITLE: TIFU by accident   \n",
       "5      -0.283909         SUBREDDIT: r/relationships\\nTITLE: I [28 M   \n",
       "6      -2.568755                        SUBREDDIT: r/AskReddit\\nTIT   \n",
       "7      -1.511305                    SUBREDDIT: r/relationships\\nTIT   \n",
       "8      -1.859813       SUBREDDIT: r/AskReddit\\nTITLE: My girlfriend   \n",
       "9      -1.856711  SUBREDDIT: r/AskReddit\\nTITLE: How do you expl...   \n",
       "10     -1.991775                     SUBREDDIT: r/AskReddit\\nTITLE:   \n",
       "11     -0.259468                      SUBREDDIT: r/AskReddit\\nTITLE   \n",
       "12     -0.089236  SUBREDDIT: r/relationships\\nTITLE: My family w...   \n",
       "13     -0.571043                   SUBREDDIT: r/legaladvice\\nTITLE:   \n",
       "14     -0.235847                     SUBREDDIT: r/AskReddit\\nTITLE:   \n",
       "15     -0.965613                                      SUBREDDIT: r/   \n",
       "\n",
       "    scores (RLHF)                                 response (best_of)  \\\n",
       "0       -0.719674  SUBREDDIT: r/AskReddit\\n classrooms is this no...   \n",
       "1        0.305112                  SUBREDDIT: r/AskReddit\\n explains   \n",
       "2       -1.110452  SUBREDDIT: r/AskReddit\\n checking app capabili...   \n",
       "3        0.465835  SUBREDDIT: r/Dogtraining\\n Various aspects of ...   \n",
       "4       -0.099994  SUBREDDIT: r/relationships\\n\"...If your hunger...   \n",
       "5        0.815666  SUBREDDIT: r/relationship_advice\\n underwent a...   \n",
       "6        1.383220                SUBREDDIT: r/AskReddit\\n configured   \n",
       "7        0.939206                       SUBREDDIT: r/AskReddit\\n 389   \n",
       "8        0.701582          SUBREDDIT: r/AskReddit\\n by venturebubb\\n   \n",
       "9       -0.890963  SUBREDDIT: r/AskReddit\\n (\"Random curiosities ...   \n",
       "10       1.354235               SUBREDDIT: r/personalfinance\\nTitles   \n",
       "11       2.002223                    SUBREDDIT: r/AskReddit\\nPartner   \n",
       "12       0.227591      SUBREDDIT: r/running\\ndiet\\nTITLE: Overweight   \n",
       "13      -0.949782          SUBREDDIT: r/relationship_advice\\n except   \n",
       "14       1.354235                  SUBREDDIT: r/AskReddit\\nTitle: As   \n",
       "15      -0.965613                                      SUBREDDIT: r/   \n",
       "\n",
       "    scores (best_of)  \n",
       "0           2.801826  \n",
       "1           2.208817  \n",
       "2           3.762428  \n",
       "3           4.343467  \n",
       "4           2.549654  \n",
       "5           2.639867  \n",
       "6           3.596242  \n",
       "7           2.758214  \n",
       "8           2.781665  \n",
       "9           2.662919  \n",
       "10          4.292090  \n",
       "11          4.770880  \n",
       "12          2.343759  \n",
       "13          3.176759  \n",
       "14          4.721159  \n",
       "15         -0.965613  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_data[\"response (ref)\"] = response_tensors_ref\n",
    "output_data[\"scores (ref)\"] = scores_ref\n",
    "output_data[\"response (RLHF)\"] = response_tensors\n",
    "output_data[\"scores (RLHF)\"] = scores\n",
    "output_data[\"response (best_of)\"] = [\n",
    "    response_tensors_best_of[i][a.argmax().item()] for i, a in enumerate(scores_best_of)\n",
    "]\n",
    "output_data[\"scores (best_of)\"] = [a.max().item() for a in scores_best_of]\n",
    "\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(output_data)\n",
    "df_results.to_csv('best_of_128.csv')\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "choi_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
