{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "\n",
    "dataset = load_dataset('CarperAI/openai_summarize_comparisons', split=\"test\")\n",
    "dataset = dataset.select(range(9000))\n",
    "\n",
    "max_length = max(len(chosen) for chosen in dataset['chosen'])\n",
    "min_length = min(len(chosen) for chosen in dataset['chosen'])\n",
    "\n",
    "\n",
    "print(f'데이터셋의 답변 최대길이 : {max_length}, 최소길이 : {min_length}')\n",
    "# 최대 : 257, 최소 : 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from trl.core import LengthSampler\n",
    "from trl.extras import BestOfNSampler\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\n",
    "def build_dataset(tokenizer, load_method=1, sample_num=9000, dataset_name=\"CarperAI/openai_summarize_comparisons\"):\n",
    "    # 데이터 로딩 9000개까지만 \n",
    "    if load_method:\n",
    "        ds = load_dataset(dataset_name, split=\"test\")\n",
    "    else:\n",
    "        ds = load_from_disk(\"test_data\")\n",
    "\n",
    "    \n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"prompt\"])\n",
    "        sample[\"prompt\"] = tokenizer.decode(sample[\"input_ids\"])\n",
    "        return sample\n",
    "    \n",
    "    ds = ds.select(range(sample_num)) \n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type=\"torch\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "ref_model_name = 'CarperAI/openai_summarize_tldr_ppo'\n",
    "reward_model = 'cjhyeok/tldr-reward_model'\n",
    "base_model = 'EleutherAI/gpt-j-6b'\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(ref_model_name)\n",
    "reward_model = pipeline(\"text-classification\", model=reward_model, tokenizer=tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 위의 샘플기준 답변 chosen 최소값 ~ 최대값\n",
    "output_min_length = 27\n",
    "output_max_length = 257\n",
    "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "\n",
    "ref_model.cuda()\n",
    "\n",
    "# callable that takes a list of raw text and returns a list of corresponding reward scores\n",
    "def queries_to_scores(list_of_strings):\n",
    "  return [output[\"score\"] for output in reward_model(list_of_strings)]\n",
    "\n",
    "\n",
    "# 결국 이방법도 github의 best_of_n과 같은 방식을 사용함\n",
    "# 기본값은 sample_size = 4, n_candidates=1 4개중 산출된 점수높은 1개를 선택하는 방식\n",
    "# https://github.com/huggingface/trl/blob/7d0a8eea4e01dd4d3247ea3608dec2ec8be10b34/trl/extras/best_of_n_sampler.py#L11\n",
    "\n",
    "# sample_size : 각 쿼리에 대해 생성할 샘플 수\n",
    "# n_candidates : 각 쿼리에 대해 반환할 후보 수\n",
    "best_of_n = BestOfNSampler(ref_model, tokenizer, queries_to_scores, length_sampler=output_length_sampler, seed=2023, sample_size =128, n_candidates =1)\n",
    "dataset = build_dataset(tokenizer,load_method=1)\n",
    "\n",
    "\n",
    "dataset.set_format(\"pandas\")\n",
    "df_batch = dataset[:]\n",
    "query_tensors = df_batch[\"input_ids\"].tolist()\n",
    "\n",
    "\n",
    "best_of_128_output = []\n",
    "for i in tqdm(range(len(query_tensors))):\n",
    "    try:\n",
    "        ans = best_of_n.generate(torch.tensor(query_tensors[i]), device=device)\n",
    "        best_of_128_output.append(ans)\n",
    "    except:\n",
    "        # 혹시모르니 에러시는 빈칸\n",
    "        best_of_128_output.append('')\n",
    "\n",
    "with open('./best_of_128_tldr.pkl', 'wb') as f:\n",
    "    pickle.dump(best_of_128_output, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
